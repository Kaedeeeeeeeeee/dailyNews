# カエデのAIニュース【2026年02月16日】

> 週明けの今日は、注目のAIニュースが13件も届いています！

## 目次

1. [Google Antigravity] Antigravityの4つのモード
2. [GitHub] GitHub CopilotがMCPに対応
3. [Kilo] Kilo GatewayがGLM-5に対応
4. [Kilo] KiloでGLM-5が無料公開
5. [Hugging Face] 数学特化モデルQED-Nano発表
6. [Hugging Face] IMO難問を解く4B推論モデル
7. [Kimi.ai] Kimi.aiがKimi Claw発表
8. [MiniMax (official)] MiniMax-M2.5の性能向上
9. [MiniMax (official)] MiniMaxの100TPS詳細
10. [MiniMax (official)] MiniMax M2.5高速版が登場
11. [MiniMax (official)] MiniMaxがOpenRoomを公開
12. [MiniMax (official)] MiniMax-2.5がローカル実行対応
13. [MiniMax (official)] MiniMax-M2.5の性能公開

---

## 1. 【Google Antigravity】Antigravityの4つのモード

Antigravityは、コードの作成やAIエージェント群の管理を最適化する4つのモードを発表しました。このエージェントはユーザー個別の好みに適応する機能を備えています。開発から高度なエージェント運用まで、用途に合わせて柔軟にモードを切り替えることで、効率的なワークフローを実現します。

https://x.com/antigravity/status/2023079862987055421

---

## 2. 【GitHub】GitHub CopilotがMCPに対応

GitHubは、GitHub Copilotのコーディングエージェントにおいて、MCP（Model Context Protocol）サーバーの設定に対応したことを発表しました。これにより、Sentryのクラッシュレポートや機能仕様書などの外部ツールを直接Copilotに接続できるようになります。従来のようにログや仕様を手動でチャットに貼り付ける必要がなくなり、エージェントが必要な情報を自ら取得して作業を行うことが可能になります。開発ワークフローの自動化と効率化を促進する重要なアップデートです。

https://x.com/github/status/2022792999676100879

---

## 3. 【Kilo】Kilo GatewayがGLM-5に対応

Kilo Gatewayが、無料のGLM-5推論機能を自社製品やOpenClawエージェントに直接統合できるようになったことを発表しました。開発者はKilo Gatewayを介することで、GLM-5の無料推論を容易に自身のプロダクトへ組み込むことが可能です。コミュニティでの活用が進むGLM-5を、より手軽に開発環境やエージェントツールへ導入できる手段を提供しています。

https://x.com/kilocode/status/2023070806943621392

---

## 4. 【Kilo】KiloでGLM-5が無料公開

AIプラットフォームのKiloは、最新のAIモデルであるGLM-5を無料で提供することを発表しました。今回の提供は、Fireworks AIおよびZhipu AI（Zai）との提携により実現しており、インフラストラクチャの最適化によってより高速なスケーリングが可能になっています。ユーザーはKiloを通じて、GLM-5の高度な機能をコストを気にせず利用できるようになります。

https://x.com/kilocode/status/2023037424377659547

---

## 5. 【Hugging Face】数学特化モデルQED-Nano発表

数学的証明の記述に特化した新しいAIモデル「QED-Nano」が発表されました。このモデルは4B（40億）パラメータという比較的小規模なサイズでありながら、GPT-OSS-120Bのような大規模モデルに匹敵する性能を発揮します。数学的な推論や証明作成において高い効率性を実現しており、リソースの限られた環境でも高度な数学的タスクの実行が期待されます。本モデルはオープンソースとして公開されています。

https://x.com/huggingface/status/2022970117504844176

---

## 6. 【Hugging Face】IMO難問を解く4B推論モデル

Hugging Faceの研究者が、国際数学オリンピック（IMO）レベルの問題を解くことができる、40億パラメータ（4B）の小型推論モデルを発表しました。このモデルは、数百万トークンのデータを用いて高度な推論プロセスを学習しています。軽量なモデルでありながら、非常に難易度の高い数学的問題に対処できる性能を備えている点が特徴です。詳細は公式ブログにて公開されています。

https://x.com/huggingface/status/2022968654460252431

---

## 7. 【Kimi.ai】Kimi.aiがKimi Claw発表

Kimi.ai（Moonshot AI）は、ブラウザ上で常駐動作する新機能「Kimi Claw」を発表しました。これはOpenClawをKimi.aiにネイティブ統合したもので、24時間365日の利用が可能です。主な特徴として、5,000以上のコミュニティスキルが集まる「ClawHub」へのアクセス、40GBの大容量クラウドストレージ、およびプロレベルのリアルタイムWeb検索機能を備えています。これにより、ユーザーはブラウザのタブから直接、高度なスキルやファイル管理、最新情報の取得を実行できるようになります。

https://x.com/Kimi_Moonshot/status/2023029674549596301

---

## 8. 【MiniMax (official)】MiniMax-M2.5の性能向上

中国のAI企業MiniMaxは、最新モデル「MiniMax-M2.5」におけるプリフィル（prefill）処理の性能情報を公開しました。推論の初期段階であるプリフィルは、モデルの全体的な応答速度を左右する重要な技術要素です。本モデルは大規模なコンテキストの処理効率に重点を置いて設計されており、開発者コミュニティからはその実用的な処理能力の高さが注目されています。このアップデートにより、複雑なプロンプトへの迅速な対応が期待されます。

https://x.com/MiniMax_AI/status/2023094815228084706

---

## 9. 【MiniMax (official)】MiniMaxの100TPS詳細

MiniMaxは、自社モデルのパフォーマンス指標である「100 TPS（Tokens Per Second）」の定義について言及しました。この数値は純粋なデコーディング速度（デコード時間のみを測定対象とした速度）を指しており、計測手法の違いが他指標との数値の差異に繋がっていると説明しています。推論性能の客観的な比較に寄与する技術情報です。

https://x.com/MiniMax_AI/status/2023094533827985600

---

## 10. 【MiniMax (official)】MiniMax M2.5高速版が登場

MiniMax社は、新型AIモデル「MiniMax-M2.5-HighSpeed」を正式にリリースしました。このモデルは次世代のエージェントアプリケーション向けに設計されており、100 TPS（Tokens Per Second）という高速な処理速度を実現しています。従来のモデルと比較して約3倍の高速化を達成しており、より迅速なレスポンスが求められるAIエージェントの開発に適しています。先行して発表されたMiniMax M2.5への高い関心に応える形で、スピード性能に特化した本モデルの提供が開始されました。

https://x.com/MiniMax_AI/status/2023066199995949350

---

## 11. 【MiniMax (official)】MiniMaxがOpenRoomを公開

MiniMax社は、AIエージェント向けプラットフォーム「OpenRoom」を発表しました。同社はこれを「エージェント・ネイティブなプレイグラウンド」に向けた第一歩であると説明しています。AIエージェントが自律的に動作する環境を提供することで、次世代のエージェント開発を加速させる狙いがあります。今回のリリースは、MiniMaxがエージェント基盤の構築に注力していることを示しています。

https://x.com/MiniMax_AI/status/2023059842542485683

---

## 12. 【MiniMax (official)】MiniMax-2.5がローカル実行対応

MiniMax-2.5がUnslothのサポートによりローカル環境で実行可能になりました。本モデルは2,300億（230B）のパラメータを持ち、700B未満のLLMにおいて最高性能を誇るとされています。大規模なモデルでありながら、Unslothの最適化技術を活用することで効率的なローカル実行が可能となりました。高い推論能力を持つ大規模言語モデルを、自身のインフラで運用したいユーザーにとって重要な技術アップデートです。

https://x.com/MiniMax_AI/status/2023030027772915865

---

## 13. 【MiniMax (official)】MiniMax-M2.5の性能公開

MiniMaxは、自社モデル「MiniMax-M2.5」において、NVIDIAのFP4精度（NVFP4）を用いた動作パフォーマンスを公開しました。2枚のRTX 6000 GPU上で動作させた際、シングルストリームで毎秒83トークンの生成速度を実現しています。また、最大32名の同時ユーザーによる推論実行を可能としています。この結果は、同モデルが高度な量子化技術により、高い計算効率とスケーラビリティを両立していることを示しています。

https://x.com/MiniMax_AI/status/2022927169614418067

---
