"""
AI Processor Module
Uses Gemini to determine AI relevance, generate summaries and titles in one call.
Optimized for parallel processing.
"""

import os
import asyncio
import json
import re
from dataclasses import dataclass
from typing import Optional
from google import genai


@dataclass
class ProcessedTweet:
    """A tweet that has been processed by AI."""
    id: str
    original_text: str
    author_username: str
    author_name: str
    url: str
    is_ai_related: bool
    summary: str
    title: str
    created_at: str
    image_urls: list[str] = None  # Twitter image URLs


class AIProcessor:
    """Processes tweets using Gemini AI with optimized single-call processing."""

    def __init__(self, api_key: str, model: str = "gemini-2.0-flash", max_concurrent: int = 10):
        self.client = genai.Client(api_key=api_key)
        self.model_name = model
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def process_tweet(self, tweet, max_summary_chars: int = 300) -> Optional[ProcessedTweet]:
        """
        Process a single tweet: check relevance, generate summary and title in ONE API call.
        
        Args:
            tweet: Tweet object from scraper
            max_summary_chars: Maximum summary length
            
        Returns:
            ProcessedTweet if AI-related, None otherwise
        """
        if not tweet.text.strip():
            return None

        prompt = f"""„ÅÇ„Å™„Åü„ÅØAI„Éã„É•„Éº„Çπ„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆ„ÉÑ„Ç§„Éº„Éà„ÇíÂàÜÊûê„Åó„ÄÅ‰æ°ÂÄ§„ÅÆ„ÅÇ„ÇãAI„Éã„É•„Éº„Çπ„Åã„Å©„ÅÜ„ÅãÂà§Êñ≠„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Äê„Çø„Çπ„ÇØ„Äë
1. „Åì„ÅÆ„ÉÑ„Ç§„Éº„Éà„Åå„Äå‰æ°ÂÄ§„ÅÆ„ÅÇ„ÇãAIË£ΩÂìÅ„ÉªÊäÄË°ì„Éã„É•„Éº„Çπ„Äç„Åã„Å©„ÅÜ„ÅãÂà§Êñ≠„Åô„Çã
2. Ë©≤ÂΩì„Åô„ÇãÂ†¥Âêà„ÅÆ„Åø„ÄÅÊó•Êú¨Ë™û„ÅßË¶ÅÁ¥Ñ„Å®„Çø„Ç§„Éà„É´„ÇíÁîüÊàê„Åô„Çã

„Äê‚úÖ Êé°Áî®„Åô„Çã„Éã„É•„Éº„ÇπÔºàis_ai_related: trueÔºâ„Äë
- Êñ∞Ë£ΩÂìÅ„ÉªÊñ∞„Çµ„Éº„Éì„Çπ„ÅÆÁô∫Ë°®Ôºà‰æãÔºöChatGPT GoÁô∫Ë°®„ÄÅClaude 3.5„É™„É™„Éº„ÇπÔºâ
- Êñ∞Ê©üËÉΩ„Éª„Ç¢„ÉÉ„Éó„Éá„Éº„ÉàÔºà‰æãÔºö„É°„É¢„É™Ê©üËÉΩËøΩÂä†„ÄÅAPIÂ§âÊõ¥Ôºâ
- ÊñôÈáëÂ§âÊõ¥„Éª„Ç≠„É£„É≥„Éö„Éº„É≥Ôºà‰æãÔºöÁÑ°Êñô„Éó„É©„É≥Êã°Â§ß„ÄÅÂÄ§‰∏ã„ÅíÔºâ
- ÊäÄË°ìÁöÑ„Å™„Éñ„É¨„Éº„ÇØ„Çπ„É´„Éº„ÉªÁ†îÁ©∂ÊàêÊûú
- ÈáçË¶Å„Å™„Éë„Éº„Éà„Éä„Éº„Ç∑„ÉÉ„Éó„ÉªÁµ±ÂêàÔºà‰æãÔºö‚óã‚óã„ÅåGPT-4„ÇíÊé°Áî®Ôºâ
- „Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„É™„É™„Éº„Çπ

„Äê‚ùå Èô§Â§ñ„Åô„Çã„Éã„É•„Éº„ÇπÔºàis_ai_related: falseÔºâ„Äë
- ‰∫∫Áâ©Èñì„ÅÆ‰∫â„ÅÑ„ÉªË´ñ‰∫â„ÉªÊâπÂà§Ôºà‰æãÔºö‚óã‚óãÊ∞è„Åå‚ñ≥‚ñ≥Ê∞è„ÇíÊâπÂà§Ôºâ
- Ë¨õÊºî„Éª„Ç§„Éô„É≥„Éà„Éª„Ç´„É≥„Éï„Ç°„É¨„É≥„Çπ„ÅÆÂëäÁü•ÔºàÂÖ∑‰ΩìÁöÑ„Å™Áô∫Ë°®ÂÜÖÂÆπ„Åå„Å™„ÅÑÂ†¥ÂêàÔºâ
- Êé°Áî®ÊÉÖÂ†±„ÉªÊ±Ç‰∫∫
- Âçò„Å™„ÇãÊÑüÊÉ≥„ÉªÊÑèË¶ã„Éª„Ç≥„É°„É≥„Éà
- ÂÖ∑‰ΩìÁöÑ„Å™ÂÜÖÂÆπ„ÅÆ„Å™„ÅÑÂÆ£‰ºù„ÉÑ„Ç§„Éº„Éà
- „É™„ÉÑ„Ç§„Éº„Éà„ÅÆ‰æùÈ†º„ÇÑ„Ç®„É≥„Ç≤„Éº„Ç∏„É°„É≥„ÉàÁ®º„Åé

„ÄêË¶ÅÁ¥Ñ„ÅÆË¶Å‰ª∂„Äë
- {max_summary_chars}ÊñáÂ≠ó‰ª•ÂÜÖ
- „Äå„Åß„Åô„Éª„Åæ„Åô„ÄçË™ø„ÅßÁµ±‰∏Ä
- Â∞ÇÈñÄÁöÑ„ÅßÂÆ¢Ë¶≥ÁöÑ„Å™Ë™ûË™ø
- ÈáçË¶Å„Å™ÊÉÖÂ†±„ÇíÂÑ™ÂÖà
- ÊäÄË°ìÁöÑ„Å™Ë©≥Á¥∞„ÇÑÊï∞Â≠ó„Çí‰øùÊåÅ
- Ëã±Ë™û„ÅÆÂõ∫ÊúâÂêçË©ûÔºàGPT-5„ÄÅClaude„ÄÅOpenAI „Å™„Å©Ôºâ„ÅØ„Åù„ÅÆ„Åæ„Åæ‰øùÊåÅ

„Äê„Çø„Ç§„Éà„É´„ÅÆË¶Å‰ª∂„Äë
- 15ÊñáÂ≠ó‰ª•ÂÜÖ
- Êó•Êú¨Ë™û„ÅßÁ∞°ÊΩî„Å´
- Ë£ΩÂìÅÂêç„ÇÑÂÖ∑‰ΩìÁöÑ„Å™ÂÜÖÂÆπ„ÇíÂê´„ÇÅ„Çã

„ÄêÂá∫ÂäõÂΩ¢Âºè„ÄëÂøÖ„Åö‰ª•‰∏ã„ÅÆJSONÂΩ¢Âºè„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
{{"is_ai_related": true/false, "summary": "Ë¶ÅÁ¥Ñ„ÉÜ„Ç≠„Çπ„Éà", "title": "„Çø„Ç§„Éà„É´"}}

‰æ°ÂÄ§„ÅÆ„ÅÇ„Çã„Éã„É•„Éº„Çπ„Åß„Å™„ÅÑÂ†¥Âêà„ÅØÔºö
{{"is_ai_related": false, "summary": "", "title": ""}}

„Äê„ÉÑ„Ç§„Éº„ÉàÂÜÖÂÆπ„Äë
Áô∫‰ø°ÂÖÉ: {tweet.author_name} (@{tweet.author_username})
ÂÜÖÂÆπ: {tweet.text}

„ÄêJSONÂá∫Âäõ„Äë"""

        async with self.semaphore:
            try:
                response = await asyncio.to_thread(
                    self.client.models.generate_content,
                    model=self.model_name,
                    contents=prompt
                )
                result_text = response.text.strip()
                
                # Extract JSON from response
                json_match = re.search(r'\{[^{}]*\}', result_text)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    result = json.loads(result_text)
                
                if not result.get("is_ai_related", False):
                    return None
                
                summary = result.get("summary", "")
                title = result.get("title", "ÊúÄÊñ∞„Éã„É•„Éº„Çπ")
                
                # Ensure summary doesn't exceed max chars
                if len(summary) > max_summary_chars:
                    summary = summary[:max_summary_chars-3] + "..."
                
                # Clean title
                title = title.strip('"\'„Äå„Äç„Äé„Äè')
                if not title:
                    title = "ÊúÄÊñ∞„Éã„É•„Éº„Çπ"

                return ProcessedTweet(
                    id=tweet.id,
                    original_text=tweet.text,
                    author_username=tweet.author_username,
                    author_name=tweet.author_name,
                    url=tweet.url,
                    is_ai_related=True,
                    summary=summary,
                    title=title,
                    created_at=tweet.created_at.isoformat() if tweet.created_at else "",
                    image_urls=tweet.image_urls if hasattr(tweet, 'image_urls') else []
                )
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è JSON parse error for @{tweet.author_username}: {e}")
                return None
            except Exception as e:
                print(f"‚ùå Processing failed for @{tweet.author_username}: {e}")
                return None

    async def batch_process(
        self,
        tweets: list,
        max_summary_chars: int = 300
    ) -> list[ProcessedTweet]:
        """
        Process multiple tweets in parallel with controlled concurrency.
        
        Args:
            tweets: List of Tweet objects
            max_summary_chars: Maximum summary length
            
        Returns:
            List of ProcessedTweet objects (only AI-related ones)
        """
        print(f"üöÄ Processing {len(tweets)} tweets in parallel (max {self.semaphore._value} concurrent)...")
        
        # Create tasks for all tweets
        tasks = [
            self.process_tweet(tweet, max_summary_chars)
            for tweet in tweets
        ]
        
        # Execute all tasks in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter out None results and exceptions
        processed = []
        for result in results:
            if isinstance(result, ProcessedTweet):
                processed.append(result)
            elif isinstance(result, Exception):
                print(f"‚ö†Ô∏è Task failed with exception: {result}")

        print(f"üìù Processed: {len(processed)}/{len(tweets)} tweets are AI-related")
        return processed


async def main():
    """Test the AI processor."""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("‚ùå Please set GEMINI_API_KEY environment variable")
        return

    processor = AIProcessor(api_key)
    
    # Create mock tweet for testing
    @dataclass
    class MockTweet:
        id: str
        text: str
        author_username: str
        author_name: str
        url: str
        created_at: None
    
    test_tweet = MockTweet(
        id="123",
        text="We're excited to announce GPT-5, our most capable model yet!",
        author_username="OpenAI",
        author_name="OpenAI",
        url="https://x.com/OpenAI/status/123",
        created_at=None
    )
    
    result = await processor.process_tweet(test_tweet)
    if result:
        print(f"‚úÖ AI-related: {result.is_ai_related}")
        print(f"üìù Title: {result.title}")
        print(f"üìÑ Summary: {result.summary}")
    else:
        print("‚ùå Not AI-related")


if __name__ == "__main__":
    asyncio.run(main())
