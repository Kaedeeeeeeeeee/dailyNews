"""
AI Processor Module
Uses Gemini to determine AI relevance, generate summaries and titles in one call.
Optimized for parallel processing.
"""

import os
import asyncio
import json
import re
from dataclasses import dataclass
from typing import Optional
from google import genai


@dataclass
class ProcessedTweet:
    """A tweet that has been processed by AI."""
    id: str
    original_text: str
    author_username: str
    author_name: str
    url: str
    is_ai_related: bool
    summary: str
    title: str
    created_at: str
    image_urls: list[str] = None  # Twitter image URLs


class AIProcessor:
    """Processes tweets using Gemini AI with optimized single-call processing."""

    def __init__(self, api_key: str, model: str = "gemini-2.0-flash", max_concurrent: int = 10):
        self.client = genai.Client(api_key=api_key)
        self.model_name = model
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def process_tweet(self, tweet, max_summary_chars: int = 300) -> Optional[ProcessedTweet]:
        """
        Process a single tweet: check relevance, generate summary and title in ONE API call.
        
        Args:
            tweet: Tweet object from scraper
            max_summary_chars: Maximum summary length
            
        Returns:
            ProcessedTweet if AI-related, None otherwise
        """
        if not tweet.text.strip():
            return None

        prompt = f"""ã‚ãªãŸã¯AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’åˆ†æã—ã€ä¾¡å€¤ã®ã‚ã‚‹AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã©ã†ã‹åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¹ã‚¯ã€‘
1. ã“ã®ãƒ„ã‚¤ãƒ¼ãƒˆãŒã€Œä¾¡å€¤ã®ã‚ã‚‹AIè£½å“ãƒ»æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ã‹ã©ã†ã‹åˆ¤æ–­ã™ã‚‹
2. è©²å½“ã™ã‚‹å ´åˆã®ã¿ã€æ—¥æœ¬èªã§è¦ç´„ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹

ã€âœ… æ¡ç”¨ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆis_ai_related: trueï¼‰ã€‘
- æ–°è£½å“ãƒ»æ–°ã‚µãƒ¼ãƒ“ã‚¹ã®ç™ºè¡¨ï¼ˆä¾‹ï¼šChatGPT Goç™ºè¡¨ã€Claude 3.5ãƒªãƒªãƒ¼ã‚¹ï¼‰
- æ–°æ©Ÿèƒ½ãƒ»ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆï¼ˆä¾‹ï¼šãƒ¡ãƒ¢ãƒªæ©Ÿèƒ½è¿½åŠ ã€APIå¤‰æ›´ï¼‰
- æ–™é‡‘å¤‰æ›´ãƒ»ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ï¼ˆä¾‹ï¼šç„¡æ–™ãƒ—ãƒ©ãƒ³æ‹¡å¤§ã€å€¤ä¸‹ã’ï¼‰
- æŠ€è¡“çš„ãªãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ãƒ»ç ”ç©¶æˆæœ
- é‡è¦ãªãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—ãƒ»çµ±åˆï¼ˆä¾‹ï¼šâ—‹â—‹ãŒGPT-4ã‚’æ¡ç”¨ï¼‰
- ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒªãƒªãƒ¼ã‚¹

ã€âŒ é™¤å¤–ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆis_ai_related: falseï¼‰ã€‘
- äººç‰©é–“ã®äº‰ã„ãƒ»è«–äº‰ãƒ»æ‰¹åˆ¤ï¼ˆä¾‹ï¼šâ—‹â—‹æ°ãŒâ–³â–³æ°ã‚’æ‰¹åˆ¤ï¼‰
- è¬›æ¼”ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã®å‘ŠçŸ¥ï¼ˆå…·ä½“çš„ãªç™ºè¡¨å†…å®¹ãŒãªã„å ´åˆï¼‰
- æ¡ç”¨æƒ…å ±ãƒ»æ±‚äºº
- å˜ãªã‚‹æ„Ÿæƒ³ãƒ»æ„è¦‹ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆ
- å…·ä½“çš„ãªå†…å®¹ã®ãªã„å®£ä¼ãƒ„ã‚¤ãƒ¼ãƒˆ
- ãƒªãƒ„ã‚¤ãƒ¼ãƒˆã®ä¾é ¼ã‚„ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç¨¼ã

ã€è¦ç´„ã®è¦ä»¶ã€‘
- {max_summary_chars}æ–‡å­—ä»¥å†…
- ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã§çµ±ä¸€
- å°‚é–€çš„ã§å®¢è¦³çš„ãªèªèª¿
- é‡è¦ãªæƒ…å ±ã‚’å„ªå…ˆ
- æŠ€è¡“çš„ãªè©³ç´°ã‚„æ•°å­—ã‚’ä¿æŒ
- è‹±èªã®å›ºæœ‰åè©ï¼ˆGPT-5ã€Claudeã€OpenAI ãªã©ï¼‰ã¯ãã®ã¾ã¾ä¿æŒ

ã€ã‚¿ã‚¤ãƒˆãƒ«ã®è¦ä»¶ã€‘
- 15æ–‡å­—ä»¥å†…
- æ—¥æœ¬èªã§ç°¡æ½”ã«
- è£½å“åã‚„å…·ä½“çš„ãªå†…å®¹ã‚’å«ã‚ã‚‹

ã€å‡ºåŠ›å½¢å¼ã€‘å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{"is_ai_related": true/false, "summary": "è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ", "title": "ã‚¿ã‚¤ãƒˆãƒ«"}}

ä¾¡å€¤ã®ã‚ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ãªã„å ´åˆã¯ï¼š
{{"is_ai_related": false, "summary": "", "title": ""}}

ã€ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã€‘
ç™ºä¿¡å…ƒ: {tweet.author_name} (@{tweet.author_username})
å†…å®¹: {tweet.text}

ã€JSONå‡ºåŠ›ã€‘"""

        async with self.semaphore:
            try:
                response = await asyncio.to_thread(
                    self.client.models.generate_content,
                    model=self.model_name,
                    contents=prompt
                )
                result_text = response.text.strip()
                
                # Extract JSON from response
                json_match = re.search(r'\{[^{}]*\}', result_text)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    result = json.loads(result_text)
                
                if not result.get("is_ai_related", False):
                    return None
                
                summary = result.get("summary", "")
                title = result.get("title", "æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹")
                
                # Ensure summary doesn't exceed max chars
                if len(summary) > max_summary_chars:
                    summary = summary[:max_summary_chars-3] + "..."
                
                # Clean title
                title = title.strip('"\'ã€Œã€ã€ã€')
                if not title:
                    title = "æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹"

                return ProcessedTweet(
                    id=tweet.id,
                    original_text=tweet.text,
                    author_username=tweet.author_username,
                    author_name=tweet.author_name,
                    url=tweet.url,
                    is_ai_related=True,
                    summary=summary,
                    title=title,
                    created_at=tweet.created_at.isoformat() if tweet.created_at else "",
                    image_urls=tweet.image_urls if hasattr(tweet, 'image_urls') else []
                )
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSON parse error for @{tweet.author_username}: {e}")
                return None
            except Exception as e:
                print(f"âŒ Processing failed for @{tweet.author_username}: {e}")
                return None

    async def batch_process(
        self,
        tweets: list,
        max_summary_chars: int = 300
    ) -> list[ProcessedTweet]:
        """
        Process multiple tweets in parallel with controlled concurrency.
        
        Args:
            tweets: List of Tweet objects
            max_summary_chars: Maximum summary length
            
        Returns:
            List of ProcessedTweet objects (only AI-related ones)
        """
        print(f"ğŸš€ Processing {len(tweets)} tweets in parallel (max {self.semaphore._value} concurrent)...")
        
        # Create tasks for all tweets
        tasks = [
            self.process_tweet(tweet, max_summary_chars)
            for tweet in tweets
        ]
        
        # Execute all tasks in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter out None results and exceptions
        processed = []
        for result in results:
            if isinstance(result, ProcessedTweet):
                processed.append(result)
            elif isinstance(result, Exception):
                print(f"âš ï¸ Task failed with exception: {result}")

        print(f"ğŸ“ Processed: {len(processed)}/{len(tweets)} tweets are AI-related")
        return processed

    async def generate_greeting(self, news_count: int, date_str: str) -> str:
        """
        Generate a personalized daily greeting.

        Args:
            news_count: Number of news items for today
            date_str: Date string (e.g., "2026å¹´1æœˆ31æ—¥")

        Returns:
            A short greeting message
        """
        prompt = f"""ã‚ãªãŸã¯AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ã‚¹ã‚¿ãƒ¼ã€Œã‚«ã‚¨ãƒ‡ã€ã§ã™ã€‚
ä»Šæ—¥ã¯{date_str}ã€{news_count}ä»¶ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã™ã€‚

èª­è€…ã¸ã®çŸ­ã„æŒ¨æ‹¶æ–‡ã‚’1æ–‡ï¼ˆ30æ–‡å­—ä»¥å†…ï¼‰ã§æ›¸ã„ã¦ãã ã•ã„ã€‚
- è¦ªã—ã¿ã‚„ã™ãã€æ˜ã‚‹ã„ãƒˆãƒ¼ãƒ³
- ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿
- å­£ç¯€ã€æ›œæ—¥ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é‡ãªã©ã‚’å‚è€ƒã«
- ä¾‹ï¼šã€Œä»Šæ—¥ã‚‚AIæ¥­ç•Œã¯è³‘ã‚„ã‹ã§ã™ã­ï¼ã€ã€Œé€±æœ«æ˜ã‘ã€æ³¨ç›®ã®ç™ºè¡¨ãŒç¶šãã¾ã™ï¼ã€

æŒ¨æ‹¶æ–‡ã®ã¿ã‚’å‡ºåŠ›ï¼š"""

        try:
            response = await asyncio.to_thread(
                self.client.models.generate_content,
                model=self.model_name,
                contents=prompt
            )
            greeting = response.text.strip().strip('ã€Œã€"\'')
            if greeting:
                return greeting
        except Exception as e:
            print(f"âš ï¸ Greeting generation failed: {e}")

        # Fallback to default greeting
        return "æœ¬æ—¥ã®AIæ¥­ç•Œã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚"


async def main():
    """Test the AI processor."""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("âŒ Please set GEMINI_API_KEY environment variable")
        return

    processor = AIProcessor(api_key)
    
    # Create mock tweet for testing
    @dataclass
    class MockTweet:
        id: str
        text: str
        author_username: str
        author_name: str
        url: str
        created_at: None
    
    test_tweet = MockTweet(
        id="123",
        text="We're excited to announce GPT-5, our most capable model yet!",
        author_username="OpenAI",
        author_name="OpenAI",
        url="https://x.com/OpenAI/status/123",
        created_at=None
    )
    
    result = await processor.process_tweet(test_tweet)
    if result:
        print(f"âœ… AI-related: {result.is_ai_related}")
        print(f"ğŸ“ Title: {result.title}")
        print(f"ğŸ“„ Summary: {result.summary}")
    else:
        print("âŒ Not AI-related")


if __name__ == "__main__":
    asyncio.run(main())
